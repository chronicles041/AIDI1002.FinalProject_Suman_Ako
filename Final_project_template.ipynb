{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tr_jEBnh-jv"
   },
   "source": [
    "# Title: DeiT: Data-efficient Image Transformers -  Training data-efficient image transformers & distillation through attention \n",
    "\n",
    "#### Group Member Names : Suman Malla [Section 1] and Ako Preiocus [Section 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeKSxMvrh-j0"
   },
   "source": [
    "### INTRODUCTION: \n",
    "This project focuses on leveraging the vision transformer model, DeiT (Data-efficient Image Transformers), published by Facebook Research in 2020. The DeiT model represents a significant advancement in image classification using transformers, offering competitive performance with reduced data and training requirements compared to traditional Convolutional Neural Networks (CNNs).\n",
    "*********************************************************************************************************************\n",
    "#### AIM : \n",
    "The aim of this project is to implement and evaluate the DeiT model using Hugging Face's transformers library, with the goal of reproducing the results from the paper and exploring the model's performance on different datasets.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### Github Repo:\n",
    "Link: facebookresearch/deit\n",
    "This repository provides the code for reproducing the results from the DeiT paper, including training and evaluation scripts.\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### DESCRIPTION OF PAPER:\n",
    "\n",
    "The paper `\"DeiT: Data-efficient Image Transformers\"` introduces a vision transformer model designed to be data-efficient, achieving state-of-the-art performance with fewer data and resources compared to conventional CNNs. The authors demonstrate that DeiT can achieve competitive accuracy on ImageNet with less data and computation.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### PROBLEM STATEMENT : \n",
    "The challenge addressed by this paper is the high data and computational requirements of traditional CNNs for image classification tasks. The goal is to reduce these requirements while maintaining or improving model performance.\n",
    "\n",
    "*********************************************************************************************************************\n",
    "#### CONTEXT OF THE PROBLEM:\n",
    "-***Background***: Traditional CNNs require large amounts of data and extensive training to achieve high performance on image classification tasks.\n",
    "-***Trends***: Recent advancements in transformer models suggest that they can achieve similar or better performance with less data.\n",
    "Challenges: Implementing and optimizing transformer models for image classification can be resource-intensive and requires careful tuning.\n",
    "*********************************************************************************************************************\n",
    "#### SOLUTION:\n",
    "The proposed solution is the DeiT model, which utilizes vision transformers to achieve high accuracy with less data. Key aspects of the solution include:\n",
    "\n",
    "- ***Data Efficiency***: DeiT models are trained with data augmentation and regularization techniques to enhance performance with limited data.\n",
    "- ***Integration with Hugging Face***: The model is supported by Hugging Face's transformers library, facilitating implementation and experimentation.\n",
    "- ***Evaluation***: The model is evaluated on the ImageNet dataset and can be tested on other datasets for further insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PIPLQ-h-j1"
   },
   "source": [
    "# Background\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "|Reference|Explanation|Dataset/Input|Weakness|\n",
    "|------|------|------|------|\n",
    "\n",
    "\n",
    "\n",
    "*********************************************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deODH3tMh-j2"
   },
   "source": [
    "# Implement paper code :\n",
    "*********************************************************************************************************************\n",
    "\n",
    "*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkHhku9h-j2"
   },
   "source": [
    "*********************************************************************************************************************\n",
    "### Contribution  Code :\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YdFCgWoh-j3"
   },
   "source": [
    "### Results :\n",
    "*******************************************************************************************************************************\n",
    "\n",
    "\n",
    "#### Observations :\n",
    "*******************************************************************************************************************************\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3JVj9dKh-j3"
   },
   "source": [
    "### Conclusion and Future Direction :\n",
    "*******************************************************************************************************************************\n",
    "#### Learnings :\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Results Discussion :\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Limitations :\n",
    "\n",
    "\n",
    "\n",
    "*******************************************************************************************************************************\n",
    "#### Future Extension :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATXtFdtBh-j4"
   },
   "source": [
    "# References:\n",
    "\n",
    "[1]:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQnMSAf-h-j4"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
